---
title: "Intelligent Systems. Hands-on 3 (NER)"
author: "Aziz Nabil"
date: "20/12/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# Needed for OutOfMemoryError: Java heap space 
#.jinit(parameters="-Xmx4g")
options(java.parameters = "- Xmx5000m")
library(rJava)
# If there are more memory problems, invoke gc() after the POS tagging
# The openNLPmodels.en library is not in CRAN; it has to be installed from another repository
#install.packages("openNLPmodels.en", repos = "http://datacube.wu.ac.at")
library(NLP) 
library(openNLP) 
library(openNLPmodels.en)
library(tm)
library(stringr)
```

###  Functions declaration :

```{r}
#getAnnotationsFromDocument
#getAnnotationsFromDocument returns annotations for the text document: word, sentence, #part-of-speech, and Penn Treebank parse annotations.
getAnnotationsFromDocument = function(doc){
  x=as.String(doc)
  sent_token_annotator <- Maxent_Sent_Token_Annotator()
  word_token_annotator <- Maxent_Word_Token_Annotator()
  pos_tag_annotator <- Maxent_POS_Tag_Annotator()
  y1 <- annotate(x, list(sent_token_annotator, word_token_annotator))
  y2 <- annotate(x, pos_tag_annotator, y1)
  gc()
  return(y2)  
} 

#getAnnotatedMergedDocument
#getAnnotatedMergedDocument returns the text document merged with the annotations.
getAnnotatedMergedDocument = function(doc,annotations){
  x=as.String(doc)
  y2w <- subset(annotations, type == "word")
  tags <- sapply(y2w$features, '[[', "POS")
  r1 <- sprintf("%s/%s", x[y2w], tags)
  r2 <- paste(r1, collapse = " ")
  return(r2)  
} 
#getAnnotatedPlainTextDocument
#returns the text document along with its annotations in an AnnotatedPlainTextDocument.
getAnnotatedPlainTextDocument = function(doc,annotations){
  x=as.String(doc)
  a = AnnotatedPlainTextDocument(x,annotations)
  return(a)  
} 
#detectPatternOnDocument
detectPatternOnDocument <- function(doc, pattern) {
  x=as.String(doc)
  res=str_match_all(x,pattern)
  
  dimrow=dim(res[[1]])[1]
  dimcol=dim(res[[1]])[2]
  
  # If there are no rows, no matches have been found
  if (dimrow == 0) {
    return(NA)
  }else{
    if (dimcol > 2){
      # If there are three or more columns, we have to paste all the groups together
      for (i in 1:dimrow) {
        res[[1]][i,2] = paste(res[[1]][i,2:dimcol], collapse = ' ')
      }
    }
    
    # We return all the results found separated by ','
    if (dimcol != 1) {
      result = paste(res[[1]][,2], collapse = ', ')
    }else{
      result = paste(res[[1]][,1], collapse = ', ')
    }
    return(result)
  }
}
#detectPatternOnDocumentWithContext
detectPatternOnDocumentWithContext <- function(doc, pattern) {
  txt=as.String(doc)
  number=50
  coord=str_locate(txt,pattern)
  res3=substr(txt,coord[1]-number,coord[2]+number)
  return (res3)
}
#detectPatternsInCorpus
detectPatternsInCorpus = function(corpus, patterns){
  vallEntities <- data.frame(matrix(NA, ncol = length(patterns)+1, 
                                    nrow = length(corpus)))
  names(vallEntities) <- c("File",patterns)
  for (i in 1:length(patterns)) {
    vallEntities[,i+1]=unlist(lapply(corpus, detectPatternOnDocument, 
                                     pattern=patterns[i]))
    }
  for (i in 1:length(corpus)) {
    vallEntities$File[i]=meta(corpus[[i]])$id
    }
  return (vallEntities)  
}
#detectPatternsInTaggedCorpus
detectPatternsInTaggedCorpus = function(corpus, taggedCorpus, patterns){
  vallEntities <- data.frame(matrix(NA, ncol = length(patterns)+1, 
                                    nrow = length(corpus)))
  names(vallEntities) <- c("File",patterns)
  for (i in 1:length(patterns)) {
    vallEntities[,i+1]=unlist(lapply(taggedCorpus, detectPatternOnDocument, 
                                     pattern=patterns[i]))
    }
  for (i in 1:length(corpus)) {
    vallEntities$File[i]=meta(corpus[[i]])$id
    }
  return (vallEntities)  
}
#countMatchesPerColumn
countMatchesPerColumn = function (df) {
  entityCountPerPattern <- data.frame(matrix(NA, ncol = 2, 
                                             nrow = length(names(df))-1))
  names(entityCountPerPattern) <- c("Entity","Count")
  
  for (i in 2:length(names(df))) {
    entityCountPerPattern$Entity[i-1] = names(df)[i]
    entityCountPerPattern$Count[i-1] = nrow(subset(df, !is.na(df[i])))
    }
  return (entityCountPerPattern)
}
#countMatchesPerRow
countMatchesPerRow = function (df) {
  entityCountPerFile <- data.frame(matrix(NA, ncol = 2, nrow = nrow(df)))
  names(entityCountPerFile) <- c("File","Count")
  
  for (i in 1:nrow(df)) {
    entityCountPerFile$File[i] = df$File[i]
    entityCountPerFile$Count[i] = length(Filter(Negate(is.na),df[i,2:length(df[i,])]))
    }
  return (entityCountPerFile[entityCountPerFile[2]!=0,])
}
#printMatchesPerPattern
printMatchesPerPattern = function (patterns, matches) {
  for (i in 1:length(patterns)){
    print(paste("PATTERN: ",patterns[i]))
    strings = matches[,i+1][!is.na(unlist(matches[,i+1]))]
    print(strings)
    print(" ") 
  }
}
#mergeAllMatchesInLists
mergeAllMatchesInLists = function (df) {
  matchesPerFile = rep(list(list()), nrow(df))
  for (i in 1:nrow(df)) {    
    matches=list()
    for (j in 2:ncol(df)){
      if (grepl(',',df[i,j])){
        b=strsplit(as.character(df[i,j]),split=',')
        for (j in 1:length(b[[1]])){
          matches= c(matches,str_trim(b[[1]][j]))
        }
      }else{
        if (!(is.na(df[i,j]))){
          matches = c(matches,str_trim(df[i,j]))
        }
      }
    }
    matches = unique(matches)
    matchesPerFile[[i]]=append(matchesPerFile[[i]],matches)
  }
  
  files = df[,1]
  matches = matchesPerFile
  
  allMatches<- data.frame(matrix(NA, ncol = 2, nrow = nrow(df)))
  names(allMatches) <- c("Files","Matches")
  
  allMatches$Files=files
  allMatches$Matches=matches
  
  return (allMatches)
}
#mergeGoldStandardInLists
mergeGoldStandardInLists = function (df) {
  matchesPerFile = rep(list(list()), nrow(df))
  
  for (i in 1:nrow(df)) {    
    matches=as.list(unlist(Filter(Negate(is.na),df[i,2:length(df)])))
    matchesPerFile[[i]]=append(matchesPerFile[[i]],matches)
  }
  
  files = df[,1]
  matches = matchesPerFile
  
  allMatches<- data.frame(matrix(NA, ncol = 2, nrow = nrow(df)))
  names(allMatches) <- c("Files","Matches")
  
  allMatches$Files=files
  allMatches$Matches=matches
  
  return (allMatches)
}
#calculateMetrics
calculateMetrics = function (matches, matches.gs) {
  
  metrics<- data.frame(matrix(NA, ncol = 3, nrow = 1))
  names(metrics) <- c("Precision","Recall","Fmeasure")
  
  numCorrect = 0
  allAnswers = 0
  possibleAnswers = 0
  
  for (i in 1:nrow(matches)) {    
    if (length(matches.gs$Matches[[i]])!=0) {
      l = str_trim(unlist(matches[i,2]))
      l.gs = unname(unlist(matches.gs[i,2]))
      intersection = intersect(l, l.gs)
      numCorrect = numCorrect + length(intersect(l, l.gs))
      allAnswers = allAnswers + length (l)
      possibleAnswers = possibleAnswers + length(l.gs)    
    }
  }
  
  metrics$Precision = numCorrect / allAnswers
  metrics$Recall = numCorrect / possibleAnswers
  
  beta = 1
  if ((metrics$Precision == 0) & (metrics$Recall == 0)) {
    metrics$Fmeasure = 0
  } else {
    metrics$Fmeasure = ((sqrt(beta)+1) * metrics$Precision * metrics$Recall) / 
      ((sqrt(beta)*metrics$Precision) + metrics$Recall)
  }
  
  return(metrics)
}
```
### loading files and inspecting documents
* Loading corpus data from a local directory

```{r}
source.pos = DirSource("E:\\intelligent systems\\unit_4\\txt_sentoken\\pos", encoding = "UTF-8")
corpus = Corpus(source.pos)
```

* getting the documents and inspect the first document.
```{r}
meta(corpus[[1]])$id
inspect(corpus[[1]])
```
### making Annotations :
* applying the getAnnotationsFromDocument function to corpus.

```{r}
annotations = lapply(corpus, getAnnotationsFromDocument)
```

* The annotations are sentence annotations generated by the head function. 

```{r}
head(annotations[[1]])
```

* Word annotations generated by tail function.
```{r}
tail(annotations[[1]])
```

* creating AnnotatedPlainTextDocuments that attach the annotations to the document and store the annotated corpus in another variable .
```{r}
corpus.tagged = Map(getAnnotatedPlainTextDocument, corpus, annotations)
inspect(corpus.tagged[[1]])
```
* storing all the annotations inline with the text and store the annotated corpus in another variable .
```{r}
corpus.taggedText = Map(getAnnotatedMergedDocument, corpus, annotations)
corpus.taggedText[[1]]
```
### Finding simple patterns
* those are some simple string patterns to try to identify people appearances.

```{r}
pattern0=c("created by")
pattern0=c(pattern0,"screenwriter[s]?")
pattern0=c(pattern0,"cinematographer")
pattern0=c(pattern0,"oscar winner")
```
* We detect those patterns in the corpus and we can see in which files they do appear.

```{r}
matches0 = detectPatternsInCorpus(corpus, pattern0)
matches0[!is.na(matches0[3]),c(1,3)]
```
* We check how many patterns we have found in each file.

```{r}
countMatchesPerRow(matches0) 
```
* And we check how many times each pattern has been found.

```{r}
countMatchesPerColumn(matches0) 
```

* And we print the context in which the patterns are found, to see if we can build better patterns.
```{r}
for (i in 1:length(pattern0)){
  print(paste("PATTERN: ",pattern0[i]))
  strings = lapply(corpus, detectPatternOnDocumentWithContext, pattern=pattern0[i])
  print(unlist(strings[!is.na(unlist(strings))]))
  print(" ")
}
```
### Find entities (complex regular expressions)
* Now we define more complex regular expressions that help identifying people appearances.
```{r}
pattern1=c("created by ([A-z]* [A-z]*)")
pattern1=c(pattern1,"created by [A-z]* [A-z]* \\( and ([A-z]* [A-z]*)")
pattern1=c(pattern1,"screenwriter[s]? ([A-z]* [A-z]*)")
pattern1=c(pattern1,"cinematographer(?: ,)? ([A-z]* [A-z]*)")
pattern1=c(pattern1,"oscar winner ([A-z]* [A-z]*)")
```
* We detect those patterns in the corpus and we can see in which files they do appear.

```{r}
matches1 = detectPatternsInCorpus(corpus, pattern1)
matches1[!is.na(matches1[4]),c(1,4)]
```
* We print the matches found per pattern.

```{r}
printMatchesPerPattern(pattern1, matches1)
```
* We check how many patterns we have found in each file.

```{r}
countMatchesPerRow(matches1) 
```
* And we check how many times each pattern has been found.

```{r}
countMatchesPerColumn(matches1) 
```
### Find entities using part-of-speech (POS) tags
* Now we include in our regular expressions part-of-speech information to avoid having incorrect answers.

```{r}
pattern2=c("created/VBN by/IN ([A-z]*)/NN ([A-z]*)/NN")
pattern2=c(pattern2,"created/VBN by/IN [A-z]*/NN [A-z]*/NN \\(/-LRB- and/CC ([A-z]*)/JJ ([A-z]*)/NN")
pattern2=c(pattern2,"screenwriter[s]?/NN[S]? ([A-z]*)/(?:NN[S]?|JJ) ([A-z]*)/(?:NN|JJ)")
pattern2=c(pattern2,"cinematographer/NN(?: ,/,)? ([A-z]*)/NN ([A-z]*)/NN")
pattern2=c(pattern2,"cinematographer/NN(?: ,/,)? ([A-z]*)/NN ([A-z]*)/IN ([A-z]*)/NN")
pattern2=c(pattern2,"oscar/NN winner/NN ([A-z]*)/VBG ([A-z]*)/NNS")
```
* We detect those patterns in the POS-tagged corpus.

```{r}
allEntities = detectPatternsInTaggedCorpus(corpus, corpus.taggedText, pattern2)
allEntities[!is.na(allEntities[4]),c(1,4)]
```
* We can also view the entities for a certain pattern.

```{r}
Filter(Negate(is.na),allEntities[[4]])
```

```{r}
printMatchesPerPattern(pattern2, allEntities)

```
* We count all the entities per pattern.
And we can also draw a histogram of the counts.

```{r}
entityCountPerPattern = countMatchesPerColumn(allEntities)
entityCountPerPattern
```
```{r}
hist(entityCountPerPattern$Count)
```
* We count all the entities per file.

And we can also draw a histogram of the counts.
```{r}
entityCountPerFile=countMatchesPerRow(allEntities)
entityCountPerFile
```
```{r}
hist(entityCountPerFile$Count)
```
### Write results to a file
* We can write our results to a CSV file, sowe can use them in other places.
```{r}
write.table(allEntities, file = "allEntities.csv", row.names = F, na="", sep=";")
```
### Compare with a gold standard
* Put all matches in a list for comparison with a gold standard.
```{r}
allMatches = mergeAllMatchesInLists(allEntities)
head(allMatches)
```
* Load the gold standard and put all gold standard matches in a list for comparison.

```{r}
goldStandard = read.table(file = "goldStandard.csv", quote = "", na.strings=c(""),
                          colClasses="character", sep=";")
allMatchesGold = mergeGoldStandardInLists(goldStandard)
head(allMatchesGold)
```
### Calculate the metrics (precision, recall, f-measure).

```{r}
metrics = calculateMetrics(allMatches, allMatchesGold)
metrics
```
The value for the precision is 0.933 wich is high However, the value for the recall is 0.005. which is bad, but at least the Precision is good.
### evaluation of patterns 

* **Answering the question** : Which patterns have worked better? Which worse? Why? 

we know the results of how much every pattern affects on the results , 
+ using simple patterns it is **screenwriter[s]?** with a count of **17** 
+ using complex regular expressions it is **screenwriter[s]? ([A-z]* [A-z]*)** with a count of **15** 
+ using using part-of-speech (POS) tags it is **screenwriter[s]?/NN[S]? ([A-z]*)/(?:NN[S]?|JJ) ([A-z]*)/(?:NN|JJ)**	with a count of **7** .
+ we can say that the best approach for a pattern is the one with a higher count. In this case,
It has detected it's the simple one with **17** entities, where some of them are not correct. However, the pattern which had least influence on the final metrics results is the one with POS tags Counted 7. 
