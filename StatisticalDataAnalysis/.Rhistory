install.packages(c("factoextra", "FactoMineR", "GGally", "gridExtra", "tidyverse"))
cereals <- read.table("cerealdata.txt", header=TRUE, as.is=TRUE, na.strings="-1")
cereals1=na.omit(cereals)
rownames(cereals1)=abbreviate(cereals1$Name)
cereal.pc=select(cereals1,calories,protein,fat,sodium,fiber,carbo,sugars,potass,shelf)
cereal.pc=select(cereals1,calories,protein,fat,sodium,fiber,carbo,sugars,potass,shelf)
rownames(cereals1)=abbreviate(cereals1$Name)
cereal.pc=select(cereals1,calories,protein,fat,sodium,fiber,carbo,sugars,potass,shelf)
rownames(cereals1)=abbreviate(cereals1$Name)
cereal.pc=select(cereals1,calories,protein,fat,sodium,fiber,carbo,sugars,potass,shelf)
cereal.pc=na.omit(cereal.pc)
head(cereal.pc)
cereal.pc$shelf=as.factor(cereal.pc$shelf)
cereals.pc=select(cereals1,calories,protein,fat,sodium,fiber,carbo,sugars,potass,shelf)
?as
?as.is
cereals1.pc=select(cereals1,calories,protein,fat,sodium,fiber,carbo,sugars,potass,shelf)
library(tidyverse) #data manipilation
library(GGally) # nice scatterplot matrix
library(FactoMineR) # PCA computation
library(factoextra) # nice plotting for PCA objects
library(gridExtra) # to build grid of plots
library(FactoMineR) # PCA computation
library(GGally) # nice scatterplot matrix
library(factoextra) # nice plotting for PCA objects
library(gridExtra) # to build grid of plots
library(tidyverse) #data manipilation
cereal.pc=select(cereals1,calories,protein,fat,sodium,fiber,carbo,sugars,potass,shelf)
cereal.pc=na.omit(cereal.pc)
head(cereal.pc)
cereal.pc$shelf=as.factor(cereal.pc$shelf)
#summary of the variables, shelf is a factor
summary(cereal.pc)
str(cereal.pc)
#covariance matrix, very different variances
cov(cereal.pc[,-9])
# Before starting with PCA, a nice scatterplot matrix
ggpairs(cereal.pc, lower = list(continuous="points",combo="facetdensity",mapping=aes(color=shelf)))
cereal_pca_r=PCA(cereal.pc,quali.sup=9,ncp=8,scale.unit=TRUE, graph=FALSE)
cereal_pca_r=PCA(cereal.pc,quali.sup=9,ncp=8,scale.unit=TRUE, graph=FALSE)
# by default dimensions 1 and 2 are plotted. We are using that option. To change them use axes=c(1,3).
# type cereal_pca_r to see the extensive list of results provided in the output of PCA()
cereal_pca_r
#summary of the numerical output
summary(cereal_pca_r)
#Working on the map of points, representation of individuals
plot(cereal_pca_r, cex=0.7)
# coloring points by variable shelf
plot(cereal_pca_r, cex=0.8, shadow=TRUE, habillage=9)
# change individual color by group
fviz_pca_ind(cereal_pca_r,  habillage="shelf")
fviz_pca_ind(cereal_pca_r,  label="none", habillage="shelf")
## Working on variables, circle of correlations
plot(cereal_pca_r, choix="var")
fviz_pca_var
## or with package factoextra
fviz_pca_var(cereal_pca_r)
# Ploting the 3 variables that contribute the most to the representation
plot(cereal_pca_r, shadow=TRUE,choix="var", select="contrib 3" )
# selecting variables by their contributions, quality of representation greater than 0.7
plot(cereal_pca_r, shadow=TRUE,choix="var", select="cos2 0.7" )
# selecting variables by their contributions, quality of representation greater than 0.7
plot(cereal_pca_r, shadow=TRUE,choix="var", select="cos2 0.7" )
# control variable colors using their contribution
fviz_pca_var(cereal_pca_r, col.var="contrib")
# Change the gradient color
fviz_pca_var(cereal_pca_r, col.var="contrib")+
scale_color_gradient2(low="white", mid="blue",
high="red", midpoint=55)+theme_bw()
# Quality of each variable representation on these two dimensions
cereal_pca_r$var$cos2[,1:2][,1]+ cereal_pca_r$var$cos2[,1:2][,2]
# First a table, then a barplot of variables contribution to each dimension with ggplot
rbind(cereal_pca_r$var$contrib, TOTAL=colSums(cereal_pca_r$var$contrib))
my.grid=expand.grid(x=rownames(var.contrib), y=colnames(var.contrib))
# First a table, then a barplot of variables contribution to each dimension with ggplot
rbind(cereal_pca_r$var$contrib, TOTAL=colSums(cereal_pca_r$var$contrib))
var.contrib=cereal_pca_r$var$contrib
my.grid=expand.grid(x=rownames(var.contrib), y=colnames(var.contrib))
my.grid=expand.grid(x=rownames(var.contrib), y=colnames(var.contrib))
my.grid$values= as.vector(var.contrib)
ggplot(my.grid, aes(x=y, y=values))+geom_bar(stat="identity", aes(fill=x), position=position_dodge())+
scale_fill_brewer(palette="Dark2")
ggplot(my.grid, aes(x=x, y=values))+geom_bar(stat="identity", aes(fill=y), position=position_dodge())
### Objects (observations) contribution
objcontrib=data.frame(C1=cereal_pca_r$ind$contrib[,1],C2=cereal_pca_r$ind$contrib[,2],n=rownames(cereal.pc))
# Barplots of object contributions to PC 1 with ggplot and package gridExtra
G1=ggplot(objcontrib[1:36,],aes(x=n, y=C1))+geom_bar(position=position_dodge(), stat="identity", fill="steelblue")+geom_text(aes(label=n), vjust=1.6, color="white", size=2.5, position=position_dodge(0.9))+geom_hline(yintercept=100/74)
G2=ggplot(objcontrib[31:74,],aes(x=n, y=C1))+geom_bar(position=position_dodge(), stat="identity", fill="steelblue")+geom_text(aes(label=n), vjust=1.6, color="white", size=2.5, position=position_dodge(0.9))+geom_hline(yintercept=100/74)
grid.arrange(G1,G2,nrow=2)
#If you want to order observations by the first component value (score), make a data frame with the two first scores, for instance
obsor=data.frame(C1=cereal_pca_r$ind$coord[,1],C2=cereal_pca_r$ind$coord[,2])
head(obsor[order(obsor[,1], decreasing=TRUE),])
# Checking some equalities
sum(cereal_pca_r$var$cos2[1,1:8])
sum(cereal_pca_r$ind$cos2[1,])
###
sum(cereal_pca_r$var$cos2[1:8,1])
mean(cereal_pca_r$ind$coord[,1]^2)
###
sum(cereal_pca_r$ind$contrib[,1])
#scree plot
par(mfrow=c(1,1))
plot(cereal_pca_r$eig[,1], type="l")
points(cereal_pca_r$eig[,1])
# scree plot (barplot type)
barplot(cereal_pca_r$eig[,1], names.arg=rownames(cereal_pca_r$eig))
#biplot
fviz_pca_biplot(cereal_pca_r)
# Eigenvectors
cereal_pca_r$svd$V
#correlations between variables and dimensions and significance tests
# For the first (second) component, are there any differences in the categorical variable?
dimdesc(cereal_pca_r,axes=c(1,2))
concat1 = cbind.data.frame(cereal.pc[,9],cereal_pca_r$ind$coord[,1:2])
boxplot(concat1[,2]~concat1[,1])
##Ellipses
#This function draws confidence ellipses around the categories of a supplementary categorical variable. The objective is to see
#whether the categories of a #categorical variable are significantly different from each other.
#It uses a data set with the categorical variable and the coordinates of the individuals on the principal components.
fviz_pca_ind(cereal_pca_r, label="none", habillage=cereal.pc$shelf,
addEllipses=TRUE, ellipse.level=0.95)
## reconstruction of the original data matrix usig 2 PC's
reconstruction1 = reconst(cereal_pca_r,ncp=2)
coeffRV(reconstruction1, cereal.pc[,1:8])
#### What follows below is very optional, not very important ############
############################################################################
### BIPLOTGUI, ONLY in WINDOWS, transform shelf again to numerical values
cereal.pc.b=cereal.pc
cereal.pc.b$shelf=as.numeric(cereal.pc.b$shelf)
Biplots(Data=cereal.pc.b[,-9])
library(gridExtra) # to build grid of plots
library(tidyverse) #data manipilation
library(GGally) # nice scatterplot matrix
library(FactoMineR) # PCA computation
library(factoextra) # nice plotting for PCA objects
library(gridExtra) # to build grid of plots
Biplots(Data=cereal.pc.b[,-9])
PCs.proportion.variation <- function(lambda, q = 1, propn, nobs)
{
den <- sum(lambda) # sum of all the eigenvalues
num <- sum(lambda[1:q]) # sum of the first q eigenvalues
if (num/den >= propn) return(1)
else {
se <- sqrt(2 * sum(lambda[-(1:q)])^2 * sum(lambda[1:q]^2) +
2 * sum(lambda[1:q])^2 * sum(lambda[-(1:q)]^2)) /
(nobs * den^2)
#asymptotic sd of the test statistic
test.stat <- (num/den - propn)/se
return(pnorm(test.stat))
}
}
lambda3=cereal_pca_r$eig[,1]
PCs.proportion.variation(lambda=lambda3, q=3,propn=0.78, nobs=65)
PCs.proportion.variation(lambda=lambda3, q=3,propn=0.77, nobs=65)
install.packages(c("car", "corrgram", "corrplot", "Hmisc", "moments", "MVN", "mvoutlier", "nortest", "ppcor", "tseries"))
# Importing data sets
medifis=read.table("medifis.txt")
colnames(medifis)=c("gender","height","weight","foot","arm","back","skull","knee")
hbat=read.csv("hbat.csv",header=TRUE,sep=",")
# if we have imported the id column as first column, delete it
head(hbat)
hbat=hbat[-1]
#libraries
library(corrplot) # to visualize correlations
library(Hmisc) # among other things tests on correlation coefficients
library(Hmisc) # among other things tests on correlation coefficients
library(corrgram) # a different way to visualiza correlations
library(corrgram) # a different way to visualiza correlations
library(ppcor) # partial correlations
library(ppcor) # partial correlations
library(nortest) # normality tests
library(tseries) # for Jarque Bera normality test
library(tseries) # for Jarque Bera normality test
library(car) # Box-Cox transformations
library(moments) # for kurtosis and skewness
library(MVN) # multivariate normality
library(mvoutlier) # multivariate outliers
#libraries
library(corrplot) # to visualize correlations
library(Hmisc) # among other things tests on correlation coefficients
library(corrgram) # a different way to visualiza correlations
library(ppcor) # partial correlations
library(nortest) # normality tests
library(tseries) # for Jarque Bera normality test
library(car) # Box-Cox transformations
library(moments) # for kurtosis and skewness
library(MVN) # multivariate normality
library(mvoutlier) # multivariate outliers
# Covariance and Correlation matrix, omitting the gender variable
cov(medifis[,-1])
r=cor(medifis[,-1])
# which variables correlate the most?
diag(r)=0
which(r==max(abs(r)), arr.ind=TRUE)
# Correlation coefficients and tests
rcorr(as.matrix(medifis[,-1]))
# Visualizing correlations: Package corrplot
corrplot(cor(medifis[,-1]))
corrplot.mixed(cor(medifis[,-1]))
corrplot.mixed(cor(medifis[,-1]), lower="number", upper="ellipse")
# reordering the correlation matrix: there are different methods. Sometimes it is
# useful for minning the hidden structure and pattern in the matrix, from a variable perspective.
corrplot.mixed(cor(medifis[,-1]),order="AOE" ) # Angular Order of the Eigenvectors
corrplot.mixed(cor(medifis[,-1]),order="FPC" ) # First Principal Component
# reordering the correlation matrix: there are different methods. Sometimes it is
# useful for minning the hidden structure and pattern in the matrix, from a variable perspective.
corrplot.mixed(cor(medifis[,-1]),order="AOE" ) # Angular Order of the Eigenvectors
corrplot.mixed(cor(medifis[,-1]),order="FPC" ) # First Principal Component
# reordering the correlation matrix: there are different methods. Sometimes it is
# useful for minning the hidden structure and pattern in the matrix, from a variable perspective.
corrplot.mixed(cor(medifis[,-1]),order="AOE" ) # Angular Order of the Eigenvectors
corrplot(cor(medifis[,-1]), order="hclust",hclust.method="ward.D", addrect=3) # Ward's hierarchical clustering method
#Visualizing correlations in different ways: Package corrgram
corrgram(medifis[,-1])
corrgram(medifis[,-1],
lower.panel=panel.shade, upper.panel=panel.pie,
diag.panel=panel.minmax, text.panel=panel.txt)
corrgram(medifis[,-1], order=TRUE,
lower.panel=panel.shade, upper.panel=panel.pie,
diag.panel=panel.minmax, text.panel=panel.txt)
corrgram(medifis[,-1], order=TRUE,
lower.panel=panel.shade, upper.panel=panel.pts,
diag.panel=panel.minmax, text.panel=panel.txt)
#Partial correlations, package ppcor. Detailed output
pcor(medifis[,-1])
matrix.partial=pcor(medifis[,-1])$estimate
#Visualizing partial correlations
corrgram(matrix.partial,
lower.panel=panel.shade, upper.panel=panel.pie,
diag.panel=panel.minmax, text.panel=panel.txt)
r2multv<-function(x){
r2s=1-1/(diag(solve(cov(x)))*diag(cov(x)))
r2s
}
#use it on data set "medifis"
r2multv(medifis[,-1])
# What is the variable "more linearly explained" by the others?
# use this function on hbat dataset
r2multv(hbat[,6:22])
medifis
# Covariance and Correlation matrix, omitting the gender variable
cov(medifis[,-1])
r=cor(medifis[,-1])
# which variables correlate the most?
diag(r)=0
which(r==max(abs(r)), arr.ind=TRUE)
# Visualizing correlations: Package corrplot
corrplot(cor(medifis[,-1]))
corrplot.mixed(cor(medifis[,-1]))
corrplot.mixed(cor(medifis[,-1]), lower="number", upper="ellipse")
# reordering the correlation matrix: there are different methods. Sometimes it is
# useful for minning the hidden structure and pattern in the matrix, from a variable perspective.
corrplot.mixed(cor(medifis[,-1]),order="AOE" ) # Angular Order of the Eigenvectors
corrplot.mixed(cor(medifis[,-1]),order="FPC" ) # First Principal Component
corrplot(cor(medifis[,-1]), order="hclust",hclust.method="ward.D", addrect=3) # Ward's hierarchical clustering method
#Visualizing correlations in different ways: Package corrgram
corrgram(medifis[,-1])
corrgram(medifis[,-1],
lower.panel=panel.shade, upper.panel=panel.pie,
diag.panel=panel.minmax, text.panel=panel.txt)
corrgram(medifis[,-1], order=TRUE,
lower.panel=panel.shade, upper.panel=panel.pie,
diag.panel=panel.minmax, text.panel=panel.txt)
corrgram(medifis[,-1], order=TRUE,
lower.panel=panel.shade, upper.panel=panel.pts,
diag.panel=panel.minmax, text.panel=panel.txt)
#Partial correlations, package ppcor. Detailed output
pcor(medifis[,-1])
matrix.partial=pcor(medifis[,-1])$estimate
#Visualizing partial correlations
corrgram(matrix.partial,
lower.panel=panel.shade, upper.panel=panel.pie,
diag.panel=panel.minmax, text.panel=panel.txt)
corrplot.mixed(matrix.partial,order="AOE" )
r2multv<-function(x){
r2s=1-1/(diag(solve(cov(x)))*diag(cov(x)))
r2s
}
#use it on data set "medifis"
r2multv(medifis[,-1])
# What is the variable "more linearly explained" by the others?
# use this function on hbat dataset
r2multv(hbat[,6:22])
#In this data set, important linear relationships are present. Let's calculate the determinant of S and R:
det(cor(hbat[,6:22]))
det(cov(hbat[,6:22]))
#Can you find the variables involved in the overall linear dependence?
eigen(cov(hbat[,6:22]))
# However, linear pairwise correlations between those variables are not very strong
cor(hbat[,c(11,17,18)])
#But R^2's are
r2multv(hbat[,c(11,17,18)])
corrplot.mixed(cor(medifis[,-1]))
# Covariance and Correlation matrix, omitting the gender variable
cov(medifis[,-1])
# which variables correlate the most?
diag(r)=0
which(r==max(abs(r)), arr.ind=TRUE)
r=cor(medifis[,-1])
# which variables correlate the most?
diag(r)=0
View(r)
# Covariance and Correlation matrix, omitting the gender variable
cov(medifis[,-1])
# which variables correlate the most?
diag(r)=0
which(r==max(abs(r)), arr.ind=TRUE)
# Correlation coefficients and tests
rcorr(as.matrix(medifis[,-1]))
# Visualizing correlations: Package corrplot
corrplot(cor(medifis[,-1]))
corrplot.mixed(cor(medifis[,-1]))
corrplot.mixed(cor(medifis[,-1]), lower="number", upper="ellipse")
# reordering the correlation matrix: there are different methods. Sometimes it is
# useful for minning the hidden structure and pattern in the matrix, from a variable perspective.
corrplot.mixed(cor(medifis[,-1]),order="AOE" ) # Angular Order of the Eigenvectors
corrplot.mixed(cor(medifis[,-1]),order="FPC" ) # First Principal Component
corrplot.mixed(cor(medifis[,-1]),order="FPC" ) # First Principal Component
corrplot(cor(medifis[,-1]), order="hclust",hclust.method="ward.D", addrect=3) # Ward's hierarchical clustering method
#Visualizing correlations in different ways: Package corrgram
corrgram(medifis[,-1])
corrgram(medifis[,-1], order=TRUE,
lower.panel=panel.shade, upper.panel=panel.pie,
diag.panel=panel.minmax, text.panel=panel.txt)
corrgram(medifis[,-1], order=TRUE,
lower.panel=panel.shade, upper.panel=panel.pts,
diag.panel=panel.minmax, text.panel=panel.txt)
#Partial correlations, package ppcor. Detailed output
pcor(medifis[,-1])
matrix.partial=pcor(medifis[,-1])$estimate
#Visualizing partial correlations
corrgram(matrix.partial,
lower.panel=panel.shade, upper.panel=panel.pie,
diag.panel=panel.minmax, text.panel=panel.txt)
corrplot.mixed(matrix.partial,order="AOE" )
r2multv<-function(x){
r2s=1-1/(diag(solve(cov(x)))*diag(cov(x)))
r2s
}
#use it on data set "medifis"
r2multv(medifis[,-1])
# What is the variable "more linearly explained" by the others?
# use this function on hbat dataset
r2multv(hbat[,6:22])
#In this data set, important linear relationships are present. Let's calculate the determinant of S and R:
det(cor(hbat[,6:22]))
det(cov(hbat[,6:22]))
#Can you find the variables involved in the overall linear dependence?
eigen(cov(hbat[,6:22]))
# However, linear pairwise correlations between those variables are not very strong
cor(hbat[,c(11,17,18)])
#But R^2's are
r2multv(hbat[,c(11,17,18)])
# Effective dependence coefficient
1-det(cor(medifis[,-1]))^{1/6}
# Testing normality assumptions
# There are numerous tests to check normality assumptions: Shapiro-Wilk's test for small samples $n<50$. Lilliefors' test (package nortest) for bigger samples.
lillie.test(hbat[,8])
shapiro.test(medifis[,3])
# Any conclusion?
#Other tests include Anderson-Darling test, Cramer Von-Misses and Jarque-Bera (tseries package, based on a joint statistics of skewness and kurtosis)
cvm.test(hbat[,8])
# Just to choose one, we will definite use Jarque-Bera's test when testing normality
jarque.bera.test(hbat[,8])
anscombe.test(hbat[,6])
#kurtosis
agostino.test(hbat[,6])
#We can try a transformation on hbat[,6] but it doesn't improve normality
jarque.bera.test(hbat[,6])
jarque.bera.test(hbat[,6]^2)
# Variable transformation to improve normality, new data rdoorcl and rdoorcl
rdoorcl=read.table("rdoorcl.txt")
rdoorop=read.table("rdoorop.txt")
qqPlot(rdoorcl$V1, dist="norm")
shapiro.test(rdoorcl$V1)
anscombe.test(rdoorcl$V1)
ggplot(rdoorcl, aes(sample=rdoorcl$V1))+stat_qq()+stat_qq_line()
ggplot(rdoorcl, aes(sample=rdoorcl$V1))+stat_qq_line()
# Power Transformations, Box-Cox transformation to improve normality, linear relationship between two variables,
# and/or constant variance. Here, we are interesting in improving normality (sometimes it is not possible)
# (package "car")
powerTransform(rdoorcl$V1)
# and more information
summary(powerTransform(rdoorcl$V1))
#We make a variable transformation using lambda=0.27
rdoorclt=bcPower(rdoorcl, lambda=0.27)
#and check if it improves normality
#Comparing both qqplots
par(mfrow=c(1,2))
qqPlot(rdoorcl$V1, dist="norm")
qqPlot(rdoorclt$V1, dist="norm")
par(mfrow=c(1,1))
#Cheking improvement of normality
jarque.bera.test(rdoorcl$V1)
jarque.bera.test(rdoorclt$V1)
shapiro.test(log(rdoorcl$V1))
shapiro.test(rdoorcl$V1)
testTransform(powerTransform(rdoorcl$V1~1), lambda=1)
testTransform(powerTransform(rdoorcl$V1~1), lambda=0.3)
# Bivariate Normality for the joint variable (rdoorcl$V1,rdoorop$V1)
# Estimating bivariate parameter (\lambda_1,\lambda_2)
powerTransform(cbind(rdoorcl$V1,rdoorop$V1))
summary(powerTransform(cbind(rdoorcl$V1,rdoorop$V1)~1))
# Transformations to Multivariate normality
# Although We can accept the logarithmic transformation for both variables (in previuos example),
# we are going to transform them with (\lambda_1,\lambda_2) values =c(0.16, 0.15).
# Defining the transformed variable with those lambdas
rdoorT=bcPower(cbind(rdoorcl$V1,rdoorop$V1), c(0.16,0.15))
# Redefining some graphical parameters to combine multiple plots into one overall graph
par(mfrow=c(1,2))
# Before
mvn(cbind(rdoorcl$V1,rdoorop$V1), mvnTest="mardia", multivariatePlot="qq")
# Redefining some graphical parameters to combine multiple plots into one overall graph
par(mfrow=c(1,2))
# Before
mvn(cbind(rdoorcl$V1,rdoorop$V1), mvnTest="mardia", multivariatePlot="qq")
# After
mvn(rdoorT, mvnTest="mardia", multivariatePlot="qq")
## Exploring multivariate Normality for (hbat[,6],hbat[7])
mvn(cbind(hbat[,6],hbat[,7]),mvnTest="mardia", multivariatePlot="qq")
summary(powerTransform(cbind(hbat[,6],hbat[,7])~1))
mvn(cbind(hbat[,6],log(hbat[,7])), mvnTest="mardia", multivariatePlot="qq")
# Univariate identification with boxplots. Package car:
par(mfrow=c(1,1))
#It gives you the observation index classified as outlier
Boxplot(hbat[,7], id.method="y")
#It gives you the observation classified as an outlier
boxplot(hbat[,7])$out
# Multivariate outliers, "by hand" with Mahalanobis distance, non-robust
# variables hbat[,6:18]
hbat618=hbat[,6:18]
# mean vector, cov matrix and squared mahalanobis distance
meanhbat618=sapply(hbat618, mean)
covhbat618=cov(hbat618)
mahalanobis618=mahalanobis(hbat618,meanhbat618,covhbat618)
mahalanobis618
# 95th percentile of a chi-squared distribution with 13 degrees of freedom (we are using 13 variables)
#Position of outliers
which(mahalanobis618 > qchisq(0.95,df=13))
#We got 6 outliers, their rows in the data set
pos=which(mahalanobis618 > 22.36)
pos
mahalanobis618[pos]
pos
mahalanobis618[pos]
## To plot outliers in a different color
x=rep(1, 100)
x[pos]=0
# We plot them on a scatterplot of variables 6 and 7 (they are outliers for the whole set of variables 6:18).
plot(hbat[,6],hbat[,7],col=x+2,pch=16)
# Visual identification, function qqPlot package car
qqPlot(mahalanobis618,dist="chisq",df=13, line="robust")
###### Multivariate outliers, Package rrcovHD, robust method
# Code O means the observations has been identified as an outlier
outlier_618=OutlierMahdist(hbat618)
###### Multivariate outliers, Package rrcovHD, robust method
# Code O means the observations has been identified as an outlier
outlier_618=OutlierMahdist(hbat618)
mvoutlier
##### pcout function, robust method based on principal components
hb618.out=pcout(hbat[,6:18], makeplot=TRUE)
# which potential outliers does it find?
which(hb618.out$wfinal01==0)
# plotting each point with each final combined weight. Small values indicate potential multivariate outliers.
plot(seq(1,100),hb618.out$wfinal)
